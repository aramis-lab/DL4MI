{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]\n",
    "In this lab we will train a conditional generative adversarial network (cGAN)\n",
    "to synthesize **T2-w MRI** from **T1-w MRI**.\n",
    "\n",
    "The outline of this lab is:\n",
    "\n",
    "1. Create a cGAN with a given architecture for the generator and for the\n",
    "discriminator.\n",
    "2. Train this cGAN on the\n",
    "[IXI dataset](https://brain-development.org/ixi-dataset/)\n",
    "to transform **T1-w MRI** into **T2-w MRI**.\n",
    "3. Evaluate the quality of the generated images using standard metrics.\n",
    "\n",
    "But first we will fetch the dataset and have a look at it to see what the\n",
    "task looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]\n",
    "# 0. Fetching the dataset\n",
    "\n",
    "The dataset can be found on this\n",
    "[GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
    "In the `size64` folder, there are 1154 files: 2 images for 577 subjects.\n",
    "The size of each image is (64, 64).\n",
    "\n",
    "Let's clone the repository and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KV3cmAW-3ULI"
   },
   "outputs": [],
   "source": [
    "# Get the dataset from the GitHub repository\n",
    "! git clone https://github.com/Easternwen/IXI-dataset.git\n",
    "\n",
    "# %% [markdown]\n",
    "# The dataset used in this lab is composed of preprocessed images from the\n",
    "# [IXI dataset](https://brain-development.org/ixi-dataset/). Two different\n",
    "# structural MRI modalities are comprised in this dataset:\n",
    "#\n",
    "# - T1 weighted images\n",
    "#\n",
    "# - T2 weighted images\n",
    "#\n",
    "# These modalities do not highlight the same tissues: for example the CSF\n",
    "# voxels are cancelled in T1 weighted imaging whereas they are highlighted by\n",
    "# the T2 weighted imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "538n0mwe7zCO",
    "outputId": "50748e0d-f8c7-4407-a0d4-cdf26834061e"
   },
   "outputs": [],
   "source": [
    "! ls ./IXI-dataset/size64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "BTWQREI29Rmt",
    "outputId": "7b6c7a7f-057e-499b-d1fd-fbe7efabe2e8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "root = \"./IXI-dataset/size64/\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(torch.load(os.path.join(root, 'sub-IXI002 - T1.pt')),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T1 slice for subject 002\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(torch.load(os.path.join(root, 'sub-IXI002 - T2.pt')),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T2 slice for subject 002\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# %% [markdown] \n",
    "# Let's create a custom `MvaDataset` class to easily have access to the data.\n",
    "# Here we don't use tsv files to split subjects between the training and the\n",
    "# test set. We only set the dataset to the `train` or `test` mode to access\n",
    "# training or test data.\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the dataset\n",
    "\n",
    "class MvaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset utility class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Path of the folder with all the images.\n",
    "\n",
    "    mode : {'train' or 'test'} (default = 'train')\n",
    "        Part of the dataset that is loaded. Use 'train' to get the training set\n",
    "        and 'test' to get the test set.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root, mode=\"train\"):\n",
    "\n",
    "        files = sorted(os.listdir(root))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in patient_id[:int(0.8*len(patient_id))]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in patient_id[int(0.8*len(patient_id)):]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "\n",
    "        t1 = torch.load(t1_path)[None, :, :]\n",
    "        t2 = torch.load(t2_path)[None, :, :]\n",
    "\n",
    "        return {\"T1\": t1, \"T2\": t2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Using this class and the `DataLoader` class from `torch.utils.data`, you can\n",
    "# easily have access to your dataset. Here is a quick example on how to use it:\n",
    "#\n",
    "# ```python\n",
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "# root = \"./IXI-dataset/size64/\"\n",
    "#\n",
    "# # Create a DataLoader instance for the training set\n",
    "# # You will get a batch of samples from the training set\n",
    "# dataloader = DataLoader(\n",
    "#     MvaDataset(root, mode=\"train\"),\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "#\n",
    "# for batch in dataloader:\n",
    "#     # batch is a dictionary with two keys:\n",
    "#     # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
    "#     # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# # 1. Creating your conditional GAN\n",
    "#\n",
    "# ## 1.1 Generator = U-Net\n",
    "#\n",
    "# For the generator we will use a U-Net where:\n",
    "#\n",
    "# * the descending blocks are convolutional layers followed by instance\n",
    "# normalization with a LeakyReLU activation function;\n",
    "#\n",
    "# * the ascending blocks are transposed convolutional layers followed by\n",
    "# instance normalization with a ReLU activation function.\n",
    "#\n",
    "# The parameters for each layer are given in the picture below.\n",
    "\n",
    "# %% [markdown] \n",
    "# <a href=\"https://ibb.co/QXBDNy3\"><img src=\"https://i.ibb.co/g614TkL/Capture-d-cran-2020-03-02-16-04-06.png\" width=\"800\" alt=\"Capture-d-cran-2020-03-02-16-04-06\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "7WHqr8UtDgUM",
    "outputId": "7a99a557-eef2-4830-f828-081a1e543e1d"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "%%html\n",
    "<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n",
    "  <b>Exercise</b>: Create a <code>GeneratorUNet</code> class to define the generator\n",
    "  with the architecture given above.\n",
    "</div>\n",
    "\n",
    "\n",
    "##############################\n",
    "#      Generator U-NET\n",
    "##############################\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "vqis-YL30eef",
    "outputId": "07ebdc54-ca65-444f-dc44-eb5d52d61f32"
   },
   "outputs": [],
   "source": [
    "# Summary of the generator\n",
    "G = GeneratorUNet().cuda()\n",
    "summary(G, (1, 64, 64) )\n",
    "\n",
    "# %% [markdown] \n",
    "# ## 1.2 Discriminator = 2D-CNN\n",
    "#\n",
    "# For the discriminator we will use a two-dimensional convolutional neural\n",
    "# network with 5 layers:\n",
    "#\n",
    "# * the first 4 layers are 2D-convolutional layers with  a LeakyReLU activation\n",
    "# function;\n",
    "#\n",
    "# * the last layer is a 2D-convolutional layer.\n",
    "#\n",
    "# The parameters for each layer are given in the figure below. Don't forget\n",
    "# that the input of the discriminator will be the generated image and the true\n",
    "# image since we are using a conditional GAN. Therefore, the number of input\n",
    "# channels for the first layer will be two (one for each image).\n",
    "\n",
    "# %% [markdown]\n",
    "# <a href=\"https://ibb.co/9b2jF0V\"><img src=\"https://i.ibb.co/hBHvPNZ/Capture-d-cran-2020-03-02-16-04-14.png\" width=\"800\" alt=\"Capture-d-cran-2020-03-02-16-04-14\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "i7UADZ7TC5qC",
    "outputId": "287755fc-1d17-46ae-cac4-5fa6c9191ac3"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "%%html\n",
    "<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n",
    "  <b>Exercise</b>: Create a <code>Discriminator</code> class to define the discriminator\n",
    "  with the architecture given above.\n",
    "</div>\n",
    "\n",
    "\n",
    "# Define the blocks used for the discriminator\n",
    "\n",
    "def discriminator_block(in_filters, out_filters):\n",
    "    \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "    layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "\n",
    "#############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, T1_img, T2_img):\n",
    "        # TODO\n",
    "        img_input = torch.cat((T1_img, T2_img), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "99IcSN3H4nDw",
    "outputId": "1e410849-bb9a-438b-e9cf-5997db6c4d4b"
   },
   "outputs": [],
   "source": [
    "# Summary of the discriminator\n",
    "D = Discriminator().cuda()\n",
    "summary(D, [(1, 64, 64), (1, 64, 64)])\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. Training our conditional GAN\n",
    "#\n",
    "# Now that we have created our generator and our discriminator, we have to\n",
    "# train them on the dataset.\n",
    "#\n",
    "# **Notations**\n",
    "#\n",
    "# * $X_{T1}$: true T1 image;\n",
    "# * $X_{T2}$: true T2 image;\n",
    "# * $\\tilde{X}_{T2}$: generated T2 image from $X_{T1}$;\n",
    "# * $\\hat{y}_{X}$: probability returned by the discriminator that the ${X}_{T2}$ is real;\n",
    "# * $\\hat{y}_{\\tilde{X}}$: probability returned by the discriminator that the $\\tilde{X}_{T2}$ is real.\n",
    "#\n",
    "# **Training the generator**\n",
    "#\n",
    "# The loss for the generator is the sum of:\n",
    "#\n",
    "# * the binary cross-entropy loss between the predicted probabilities of the\n",
    "# generated images and positive labels,\n",
    "# * the pixel-wise mean absolute error between the generated image and the true\n",
    "# image.\n",
    "#\n",
    "# For one sample, it is then:\n",
    "# $$\n",
    "# \\ell_G = - \\log(\\hat{y}_{\\tilde{X}}) + \\lambda * \\text{MAE}(X_{T2}, \\tilde{X}_{T2})\n",
    "# $$\n",
    "#\n",
    "# **Training the discriminator**\n",
    "#\n",
    "# The loss for the generator is the mean of:\n",
    "#\n",
    "# * the binary cross-entropy loss between the predicted probabilities of the\n",
    "# generated images and negative labels,\n",
    "# * the binary cross-entropy loss between the predicted probabilities\n",
    "# of the true images and positive labels.\n",
    "#\n",
    "# For one sample, it is then:\n",
    "# $$\n",
    "# \\ell_D = - 0.5 * \\log(\\hat{y}_{X}) - 0.5 * \\log(1 - \\hat{y}_{\\tilde{X}})\n",
    "# $$\n",
    "#\n",
    "# **Training phase**\n",
    "#\n",
    "# The generator and the discriminator are trained simultaneously, which makes\n",
    "# the training phase look like this:\n",
    "#\n",
    "# ```\n",
    "# # For each epoch\n",
    "#\n",
    "#     # For each batch\n",
    "#\n",
    "#         # Generate fake images for all the images in this batch\n",
    "#\n",
    "#         # Compute the loss for the generator and perform one optimization step\n",
    "#\n",
    "#         # Compute the loss for the discriminator and perform one optimization step\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "SmezSS4UCXoL",
    "lines_to_next_cell": 1,
    "outputId": "9597f4cb-25b6-4ceb-c919-632833f769f8"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "%%html\n",
    "<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n",
    "  <b>Exercise</b>: We provide below a template to train our conditional GAN\n",
    "  on the dataset. Fill in the missing parts and look at the generated images.\n",
    "</div>\n",
    "\n",
    "\n",
    "def train(train_loader, test_loader, num_epoch=500,\n",
    "          lr=0.0001, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Method used to train a generator in an adversarial framework.\n",
    "\n",
    "    Args:\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
    "        test_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
    "        num_epoch: (int) number of epochs performed during training\n",
    "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
    "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "\n",
    "    Returns:\n",
    "        generator: (nn.Module) the trained generator\n",
    "    \"\"\"\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    print(\"cuda %s\" % cuda)  # check if GPU is used\n",
    "\n",
    "    # Tensor type (put everything on GPU if possible)\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(\"./images\"):\n",
    "        os.makedirs(\"./images\")\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_GAN = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
    "    criterion_pixelwise = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "\n",
    "    lambda_GAN = 1  # Weights criterion_GAN in the generator loss\n",
    "    lambda_pixel = 0  # Weights criterion_pixelwise in the generator loss\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    generator = GeneratorUNet()\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    if cuda:\n",
    "        generator = generator.cuda()\n",
    "        discriminator = discriminator.cuda()\n",
    "        criterion_GAN.cuda()\n",
    "        criterion_pixelwise.cuda()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(),\n",
    "                                   lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(),\n",
    "                                   lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    def sample_images(epoch):\n",
    "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "        imgs = next(iter(test_loader))\n",
    "        real_A = Variable(imgs[\"T1\"].type(Tensor))\n",
    "        real_B = Variable(imgs[\"T2\"].type(Tensor))\n",
    "        fake_B = generator(real_A)\n",
    "        img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "        save_image(img_sample, \"./images/epoch-%s.png\" % epoch,\n",
    "                   nrow=5, normalize=True)\n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_A = Variable(batch[\"T1\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"T2\"].type(Tensor))\n",
    "\n",
    "            # Create labels\n",
    "            valid = Variable(Tensor(np.ones((real_B.size(0), 1, 1, 1))),\n",
    "                             requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_B.size(0), 1, 1, 1))),\n",
    "                            requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # GAN loss\n",
    "            ...\n",
    "            loss_GAN = ...\n",
    "            loss_pixel = ...\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = lambda_GAN * loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            ...\n",
    "            loss_real = ... # loss on real inputs\n",
    "\n",
    "            # Fake loss\n",
    "            ...\n",
    "            loss_fake = ... # loss on generated inputs\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_loader) + i\n",
    "            batches_left = num_epoch * len(train_loader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] \"\n",
    "                \"\\r[G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_pixel.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save images at the end of each epoch\n",
    "        sample_images(epoch)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "uv48odyq4tUA",
    "outputId": "2f16e86a-c8be-4cfc-b61b-ec833067a58a"
   },
   "outputs": [],
   "source": [
    "root = \"./IXI-dataset/size64/\"\n",
    "\n",
    "# Parameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 40\n",
    "train_loader = DataLoader(MvaDataset(root, mode=\"train\"),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(MvaDataset(root, mode=\"test\"),\n",
    "                         batch_size=5,\n",
    "                         shuffle=False)\n",
    "\n",
    "num_epoch = 20\n",
    "\n",
    "generator = train(train_loader, test_loader, num_epoch=num_epoch,\n",
    "                  lr=lr, beta1=beta1, beta2=beta2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Reading an image saved as a png file\n",
    "im = img.imread('./images/epoch-%s.png' % (num_epoch - 1))\n",
    "plt.imshow(np.swapaxes(im, 0, 1))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. Evaluating the quality of the generated images\n",
    "#\n",
    "# After doing visual quality control, it is a good idea to quantify the quality\n",
    "# of the generated images using specific metrics. The most popular metrics are\n",
    "# Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) and Structural\n",
    "# Similarity index (SSIM):\n",
    "# * MSE = $ \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m (T_{ij} - G_{ij}) $\n",
    "#\n",
    "# * PSNR = $10 \\log_{10\u001f} \\left( \\frac{MAX_I^2}{MSE} \\right) $ where $MAX_I^2$\n",
    "# is the maximum possible value of the image (equal to 1 in our case since the\n",
    "# images are scaled in range $[-1, 1]$). The higher, the better.\n",
    "#\n",
    "# * SSIM = $ \\frac{(2 \\mu_T \\mu_G + C_1)(2 \\sigma_{TG} + C_2)}{(\\mu_T^2 +\n",
    "# \\mu_G^2 + C_1)(\\sigma_T^2 + \\sigma_G^2 + C2)} $ where $\\mu$ and $\\sigma$ are\n",
    "# the mean value and standard deviation of an image respectively and $C_1$ and\n",
    "# $C_2$ are two positive constants (one can take $C_1=0.01$ and $C_2=0.03$).\n",
    "#\n",
    "# To better understand the differences between these metrics:\n",
    "# https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "QMnEX2hUI5X7",
    "outputId": "8d1c4c36-eeee-489d-d923-8d17cd023835"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "%%html\n",
    "<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n",
    "  <b>Exercise</b>: Define a function for each metric mentioned above and\n",
    "  evaluate the quality of the generated images on the training and test\n",
    "  sets. Compute the metrics for each image individually and find the best\n",
    "  and worst generated images according to these metrics.\n",
    "</div>\n",
    "\n",
    "def MSE(image_true, image_generated):\n",
    "    # TODO\n",
    "\n",
    "def PSNR(image_true, image_generated):\n",
    "    # TODO\n",
    "\n",
    "def SSIM(image_true, image_generated, C1=0.01, C2=0.03):\n",
    "    # TODO\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_metrics(dataloader):\n",
    "\n",
    "    res = []\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Inputs T1-w and T2-w\n",
    "        real_A = Variable(batch[\"T1\"].type(Tensor), requires_grad=False)\n",
    "        real_B = Variable(batch[\"T2\"].type(Tensor), requires_grad=False)\n",
    "        fake_B = Variable(generator(real_A), requires_grad=False)\n",
    "\n",
    "        mse = MSE(real_B, fake_B).item()\n",
    "        psnr = PSNR(real_B, fake_B).item()\n",
    "        ssim = SSIM(real_B, fake_B).item()\n",
    "\n",
    "        res.append([mse, psnr, ssim])\n",
    "\n",
    "    df = pd.DataFrame(res, columns=['MSE', 'PSNR', 'SSIM'])\n",
    "    return df\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MvaDataset(root=root, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    MvaDataset(root=root, mode=\"test\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "df_train = compute_metrics(train_loader)\n",
    "df_test = compute_metrics(test_loader)\n",
    "\n",
    "df_train\n",
    "\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
