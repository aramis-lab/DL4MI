{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will train a conditional generative adversarial network (cGAN)\n",
    "to synthesize **T2-w MRI** from **T1-w MRI**.\n",
    "\n",
    "The outline of this lab is:\n",
    "\n",
    "1. Create a cGAN with a given architecture for the generator and for the\n",
    "discriminator.\n",
    "2. Train this cGAN on the\n",
    "[IXI dataset](https://brain-development.org/ixi-dataset/)\n",
    "to transform **T1-w MRI** into **T2-w MRI**.\n",
    "3. Evaluate the quality of the generated images using standard metrics.\n",
    "\n",
    "But first we will fetch the dataset and have a look at it to see what the\n",
    "task looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fetching the dataset\n",
    "\n",
    "The dataset can be found on this\n",
    "[GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
    "In the `size64` folder, there are 1154 files: 2 images for 577 subjects.\n",
    "The size of each image is (64, 64).\n",
    "\n",
    "Let's clone the repository and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset from the GitHub repository\n",
    "! git clone https://github.com/Easternwen/IXI-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this lab is composed of preprocessed images from the\n",
    "[IXI dataset](https://brain-development.org/ixi-dataset/). Two different\n",
    "structural MRI modalities are comprised in this dataset:\n",
    "\n",
    "- T1 weighted images\n",
    "\n",
    "- T2 weighted images\n",
    "\n",
    "These modalities do not highlight the same tissues: for example the CSF\n",
    "voxels are cancelled in T1 weighted imaging whereas they are highlighted by\n",
    "the T2 weighted imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "root = \"./IXI-dataset/size64/\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(torch.load(os.path.join(root, 'sub-IXI002 - T1.pt')),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T1 slice for subject 002\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(torch.load(os.path.join(root, 'sub-IXI002 - T2.pt')),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T2 slice for subject 002\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a custom `IXIDataset` class to easily have access to the data.\n",
    "Here we don't use tsv files to split subjects between the training and the\n",
    "test set. We only set the dataset to the `train` or `test` mode to access\n",
    "training or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class IXIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset utility class.\n",
    "\n",
    "    Args:\n",
    "        root: (str) Path of the folder with all the images.\n",
    "        mode : {'train' or 'test'} Part of the dataset that is loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root, mode=\"train\"):\n",
    "\n",
    "        files = sorted(os.listdir(root))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in patient_id[:int(0.8*len(patient_id))]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in patient_id[int(0.8*len(patient_id)):]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "\n",
    "        t1 = torch.load(t1_path)[None, :, :]\n",
    "        t2 = torch.load(t2_path)[None, :, :]\n",
    "\n",
    "        return {\"T1\": t1, \"T2\": t2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this class and the `DataLoader` class from `torch.utils.data`, you can\n",
    "easily have access to your dataset. Here is a quick example on how to use it:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = \"./IXI-dataset/size64/\"\n",
    "\n",
    "# Create a DataLoader instance for the training set\n",
    "# You will get a batch of samples from the training set\n",
    "dataloader = DataLoader(\n",
    "    IXIDataset(root, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    # batch is a dictionary with two keys:\n",
    "    # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
    "    # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating your conditional GAN\n",
    "\n",
    "## 1.1 Generator = U-Net\n",
    "\n",
    "For the generator we will use a U-Net where:\n",
    "\n",
    "* the descending blocks are convolutional layers followed by instance\n",
    "normalization with a LeakyReLU activation function;\n",
    "\n",
    "* the ascending blocks are transposed convolutional layers followed by\n",
    "instance normalization with a ReLU activation function.\n",
    "\n",
    "The parameters for each layer are given in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/QXBDNy3\"><img src=\"https://i.ibb.co/g614TkL/Capture-d-cran-2020-03-02-16-04-06.png\" width=\"800\" alt=\"Capture-d-cran-2020-03-02-16-04-06\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>: Create a <code>GeneratorUNet</code> class to define the\n",
    "generator with the architecture given above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the blocks used for U-Net\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetDown, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.LeakyReLU(0.2)\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_size, out_size, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#      Generator U-NET\n",
    "##############################\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512)\n",
    "        self.down5 = UNetDown(512, 512)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512)\n",
    "        self.up2 = UNetUp(1024, 256)\n",
    "        self.up3 = UNetUp(512, 128)\n",
    "        self.up4 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = FinalLayer(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "\n",
    "        u1 = self.up1(d5)\n",
    "        u2 = self.up2(u1, d4)\n",
    "        u3 = self.up3(u2, d3)\n",
    "        u4 = self.up4(u3, d2)\n",
    "\n",
    "        return self.final(u4, d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Summary of the generator\n",
    "G = GeneratorUNet().cuda()\n",
    "summary(G, (1, 64, 64) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Discriminator = 2D-CNN\n",
    "\n",
    "For the discriminator we will use a two-dimensional convolutional neural\n",
    "network with 5 layers:\n",
    "\n",
    "* the first 4 layers are 2D-convolutional layers with  a LeakyReLU activation\n",
    "function;\n",
    "\n",
    "* the last layer is a 2D-convolutional layer.\n",
    "\n",
    "The parameters for each layer are given in the figure below. Don't forget\n",
    "that the input of the discriminator will be the generated image and the true\n",
    "image since we are using a conditional GAN. Therefore, the number of input\n",
    "channels for the first layer will be two (one for each image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/9b2jF0V\"><img src=\"https://i.ibb.co/hBHvPNZ/Capture-d-cran-2020-03-02-16-04-14.png\" width=\"800\" alt=\"Capture-d-cran-2020-03-02-16-04-14\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Exercise</b>: Create a <code>Discriminator</code> class to define the\n",
    " discriminator with the architecture given above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the blocks used for the discriminator\n",
    "\n",
    "def discriminator_block(in_filters, out_filters):\n",
    "    \"\"\"Return downsampling layers of each discriminator block\"\"\"\n",
    "    layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "\n",
    "#############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.extend(discriminator_block(in_channels*2, 64))\n",
    "        layers.extend(discriminator_block(64, 128))\n",
    "        layers.extend(discriminator_block(128, 256))\n",
    "        layers.extend(discriminator_block(256, 512))\n",
    "        layers.append(nn.Conv2d(512, 1, 4, padding=0))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Summary of the discriminator\n",
    "D = Discriminator().cuda()\n",
    "summary(D, [(1, 64, 64), (1, 64, 64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training our conditional GAN\n",
    "\n",
    "Now that we have created our generator and our discriminator, we have to\n",
    "train them on the dataset.\n",
    "\n",
    "**Notations**\n",
    "\n",
    "* $X_{T1}$: true T1 image;\n",
    "* $X_{T2}$: true T2 image;\n",
    "* $\\tilde{X}_{T2}$: generated T2 image from $X_{T1}$;\n",
    "* $\\hat{y}_{X}$: probability returned by the discriminator that the ${X}_{T2}$ is real;\n",
    "* $\\hat{y}_{\\tilde{X}}$: probability returned by the discriminator that the $\\tilde{X}_{T2}$ is real.\n",
    "\n",
    "**Training the generator**\n",
    "\n",
    "The loss for the generator is the sum of:\n",
    "\n",
    "* the binary cross-entropy loss between the predicted probabilities of the\n",
    "generated images and positive labels,\n",
    "* the pixel-wise mean absolute error between the generated image and the true\n",
    "image.\n",
    "\n",
    "For one sample, it is then:\n",
    "$$\n",
    "\\ell_G = - \\log(\\hat{y}_{\\tilde{X}}) + \\lambda * \\text{MAE}(X_{T2}, \\tilde{X}_{T2})\n",
    "$$\n",
    "\n",
    "**Training the discriminator**\n",
    "\n",
    "The loss for the generator is the mean of:\n",
    "\n",
    "* the binary cross-entropy loss between the predicted probabilities of the\n",
    "generated images and negative labels,\n",
    "* the binary cross-entropy loss between the predicted probabilities\n",
    "of the true images and positive labels.\n",
    "\n",
    "For one sample, it is then:\n",
    "$$\n",
    "\\ell_D = - 0.5 * \\log(\\hat{y}_{X}) - 0.5 * \\log(1 - \\hat{y}_{\\tilde{X}})\n",
    "$$\n",
    "\n",
    "**Training phase**\n",
    "\n",
    "The generator and the discriminator are trained simultaneously, which makes\n",
    "the training phase look like this:\n",
    "\n",
    "```\n",
    "# For each epoch\n",
    "\n",
    "    # For each batch\n",
    "\n",
    "        # Generate fake images for all the images in this batch\n",
    "\n",
    "        # Compute the loss for the generator and perform one optimization step\n",
    "\n",
    "        # Compute the loss for the discriminator and perform one optimization step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Exercise</b>: We provide below a template to train our conditional GAN\n",
    " on the dataset. Fill in the missing parts and look at the generated images.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, num_epoch=500,\n",
    "          lr=0.0001, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Method used to train a generator in an adversarial framework.\n",
    "\n",
    "    Args:\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
    "        test_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
    "        num_epoch: (int) number of epochs performed during training\n",
    "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
    "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "\n",
    "    Returns:\n",
    "        generator: (nn.Module) the trained generator\n",
    "    \"\"\"\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    print(\"cuda %s\" % cuda)  # check if GPU is used\n",
    "\n",
    "    # Tensor type (put everything on GPU if possible)\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(\"./images\"):\n",
    "        os.makedirs(\"./images\")\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_GAN = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
    "    criterion_pixelwise = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "\n",
    "    lambda_GAN = 1  # Weights criterion_GAN in the generator loss\n",
    "    lambda_pixel = 0  # Weights criterion_pixelwise in the generator loss\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    generator = GeneratorUNet()\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    if cuda:\n",
    "        generator = generator.cuda()\n",
    "        discriminator = discriminator.cuda()\n",
    "        criterion_GAN.cuda()\n",
    "        criterion_pixelwise.cuda()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(),\n",
    "                                   lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(),\n",
    "                                   lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    def sample_images(epoch):\n",
    "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "        imgs = next(iter(test_loader))\n",
    "        real_A = Variable(imgs[\"T1\"].type(Tensor))\n",
    "        real_B = Variable(imgs[\"T2\"].type(Tensor))\n",
    "        fake_B = generator(real_A)\n",
    "        img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "        save_image(img_sample, \"./images/epoch-%s.png\" % epoch,\n",
    "                   nrow=5, normalize=True)\n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_A = Variable(batch[\"T1\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"T2\"].type(Tensor))\n",
    "\n",
    "            # Create labels\n",
    "            valid = Variable(Tensor(np.ones((real_B.size(0), 1, 1, 1))),\n",
    "                             requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_B.size(0), 1, 1, 1))),\n",
    "                            requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = generator(real_A)\n",
    "            pred_fake = discriminator(fake_B, real_A)\n",
    "            loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "\n",
    "            # L1 loss\n",
    "            loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "            loss_G.backward()\n",
    "\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            pred_real = discriminator(real_B, real_A)\n",
    "            loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "            # Fake loss\n",
    "            pred_fake = discriminator(fake_B.detach(), real_A)\n",
    "            loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_loader) + i\n",
    "            batches_left = num_epoch * len(train_loader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] \"\n",
    "                \"\\r[G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_pixel.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save images at the end of each epoch\n",
    "        sample_images(epoch)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 40\n",
    "train_loader = DataLoader(IXIDataset(root, mode=\"train\"),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(IXIDataset(root, mode=\"test\"),\n",
    "                         batch_size=5,\n",
    "                         shuffle=False)\n",
    "\n",
    "num_epoch = 20\n",
    "\n",
    "generator = train(train_loader, test_loader, num_epoch=num_epoch,\n",
    "                  lr=lr, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as img\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Reading an image saved as a png file\n",
    "im = img.imread('./images/epoch-%s.png' % (num_epoch - 1))\n",
    "plt.imshow(np.swapaxes(im, 0, 1))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the quality of the generated images\n",
    "\n",
    "After doing visual quality control, it is a good idea to quantify the quality\n",
    "of the generated images using specific metrics. The most popular metrics are\n",
    "Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) and Structural\n",
    "Similarity index (SSIM):\n",
    "* MSE = $ \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m (T_{ij} - G_{ij}) $\n",
    "\n",
    "* PSNR = $10 \\log_{10\u001f} \\left( \\frac{MAX_I^2}{MSE} \\right) $ where $MAX_I^2$\n",
    "is the maximum possible value of the image (equal to 1 in our case since the\n",
    "images are scaled in range $[-1, 1]$). The higher, the better.\n",
    "\n",
    "* SSIM = $ \\frac{(2 \\mu_T \\mu_G + C_1)(2 \\sigma_{TG} + C_2)}{(\\mu_T^2 +\n",
    "\\mu_G^2 + C_1)(\\sigma_T^2 + \\sigma_G^2 + C2)} $ where $\\mu$ and $\\sigma$ are\n",
    "the mean value and standard deviation of an image respectively and $C_1$ and\n",
    "$C_2$ are two positive constants (one can take $C_1=0.01$ and $C_2=0.03$).\n",
    "\n",
    "To better understand the differences between these metrics:\n",
    "https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Exercise</b>: Define a function for each metric mentioned above and\n",
    " evaluate the quality of the generated images on the training and test\n",
    " sets. Compute the metrics for each image individually and find the best\n",
    " and worst generated images according to these metrics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def MSE(image_true, image_generated):\n",
    "    \"\"\"Compute mean squared error.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "\n",
    "    Returns:\n",
    "        mse: (float) mean squared error\n",
    "    \"\"\"\n",
    "    return ((image_true - image_generated) ** 2).mean()\n",
    "\n",
    "\n",
    "def PSNR(image_true, image_generated):\n",
    "    \"\"\"\"Compute peak signal-to-noise ratio.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "\n",
    "    Returns:\n",
    "        psnr: (float) peak signal-to-noise ratio\"\"\"\n",
    "    mse = MSE(image_true, image_generated).cpu()\n",
    "    return -10 * np.log10(mse)\n",
    "\n",
    "\n",
    "def SSIM(image_true, image_generated, C1=0.01, C2=0.03):\n",
    "    \"\"\"Compute structural similarity index.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "        C1: (float) variable to stabilize the denominator\n",
    "        C2: (float) variable to stabilize the denominator\n",
    "\n",
    "    Returns:\n",
    "        ssim: (float) mean squared error\"\"\"\n",
    "    mean_true = image_true.mean()\n",
    "    mean_generated = image_generated.mean()\n",
    "    std_true = image_true.std()\n",
    "    std_generated = image_generated.std()\n",
    "    covariance = (\n",
    "        (image_true - mean_true) * (image_generated - mean_generated)).mean()\n",
    "\n",
    "    numerator = (2 * mean_true * mean_generated + C1) * (2 * covariance + C2)\n",
    "    denominator = ((mean_true ** 2 + mean_generated ** 2 + C1) *\n",
    "                   (std_true ** 2 + std_generated ** 2 + C2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_metrics(dataloader):\n",
    "\n",
    "    res = []\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Inputs T1-w and T2-w\n",
    "        real_A = Variable(batch[\"T1\"].type(Tensor), requires_grad=False)\n",
    "        real_B = Variable(batch[\"T2\"].type(Tensor), requires_grad=False)\n",
    "        fake_B = Variable(generator(real_A), requires_grad=False)\n",
    "\n",
    "        mse = MSE(real_B, fake_B).item()\n",
    "        psnr = PSNR(real_B, fake_B).item()\n",
    "        ssim = SSIM(real_B, fake_B).item()\n",
    "\n",
    "        res.append([mse, psnr, ssim])\n",
    "\n",
    "    df = pd.DataFrame(res, columns=['MSE', 'PSNR', 'SSIM'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    IXIDataset(root=root, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    IXIDataset(root=root, mode=\"test\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "df_train = compute_metrics(train_loader)\n",
    "df_test = compute_metrics(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
