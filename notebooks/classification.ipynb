{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Classification of Alzheimer's disease diagnosis\n",
    "#\n",
    "# The goal of this lab session is to train a network that will perform a binary\n",
    "# classification between control participants and patients that are affected by\n",
    "# Alzheimer's disease. The input of the network is a neuroimaging modality: the\n",
    "# T1 weighted MRI. In this project we use the [pytorch\n",
    "# library](https://pytorch.org/).\n",
    "#\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from time import time\n",
    "from os import path\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]\n",
    "## Database\n",
    "\n",
    "In this session we use the images from a public research project:\n",
    "[OASIS-1](https://www.oasis-brains.org/#data).\n",
    "Two labels exist in this dataset:\n",
    "- CN (Cognitively Normal) for healthy participants.\n",
    "- AD (Alzheimer's Disease) for patients affected by Alzheimer's disease.\n",
    "\n",
    "All the preprocessed images we use were put on github, run the following\n",
    "command to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "XJ9xFm1nt4ed",
    "outputId": "7a6f62f2-b3c5-4e73-9a03-fedfe536f018"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/14thibea/OASIS-1_dataset.git\n",
    "\n",
    "# [markdown]\n",
    "# One crucial step before training a neural network is to check the dataset.\n",
    "# Are the classes balanced? Are there biases in the dataset that may\n",
    "# differentiate the labels?\n",
    "#\n",
    "# Here we will focus on the demographics (age, sex and level of education) and\n",
    "# two cognitive scores:\n",
    "#\n",
    "# - The MMS (Mini Mental State), rated between 0 (no correct answer) to 30\n",
    "# (healthy subject).\n",
    "# - The CDR (Clinical Dementia Rating), that is null if the participant is\n",
    "# non-demented and of 0.5, 1, 2 and 3 for very mild, mild, moderate and severe\n",
    "# dementia, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Moe2GRGkPyJJ",
    "outputId": "925a346b-36ff-41c9-8dd2-7e0a2b659595"
   },
   "outputs": [],
   "source": [
    "# Load the complete dataset\n",
    "OASIS_df = pd.read_csv('OASIS-1_dataset/tsv_files/lab_1/OASIS_BIDS.tsv', sep='\\t')\n",
    "\n",
    "# Study the characteristics of the AD & CN populations (age, sex, MMS, cdr_global)\n",
    "def characteristics_table(df, merged_df):\n",
    "    \"\"\"Creates a DataFrame that summarizes the characteristics of the DataFrame df\"\"\"\n",
    "    diagnoses = np.unique(df.diagnosis.values)\n",
    "    population_df = pd.DataFrame(index=diagnoses,\n",
    "                                columns=['N', 'age', '%sexF', 'education',\n",
    "                                         'MMS', 'CDR=0', 'CDR=0.5', 'CDR=1', 'CDR=2'])\n",
    "    merged_df = merged_df.set_index(['participant_id', 'session_id'], drop=True)\n",
    "    df = df.set_index(['participant_id', 'session_id'], drop=True)\n",
    "    sub_merged_df = merged_df.loc[df.index]\n",
    "    \n",
    "    for diagnosis in population_df.index.values:\n",
    "        diagnosis_df = sub_merged_df[df.diagnosis == diagnosis]\n",
    "        population_df.loc[diagnosis, 'N'] = len(diagnosis_df)\n",
    "        # Age\n",
    "        mean_age = np.mean(diagnosis_df.age_bl)\n",
    "        std_age = np.std(diagnosis_df.age_bl)\n",
    "        population_df.loc[diagnosis, 'age'] = '%.1f ± %.1f' % (mean_age, std_age)\n",
    "        # Sex\n",
    "        population_df.loc[diagnosis, '%sexF'] = round((len(diagnosis_df[diagnosis_df.sex == 'F']) / len(diagnosis_df)) * 100, 1)\n",
    "        # Education level\n",
    "        mean_education_level = np.nanmean(diagnosis_df.education_level)\n",
    "        std_education_level = np.nanstd(diagnosis_df.education_level)\n",
    "        population_df.loc[diagnosis, 'education'] = '%.1f ± %.1f' % (mean_education_level, std_education_level)\n",
    "        # MMS\n",
    "        mean_MMS = np.mean(diagnosis_df.MMS)\n",
    "        std_MMS = np.std(diagnosis_df.MMS)\n",
    "        population_df.loc[diagnosis, 'MMS'] = '%.1f ± %.1f' % (mean_MMS, std_MMS)\n",
    "        # CDR\n",
    "        for value in ['0', '0.5', '1', '2']:\n",
    "          population_df.loc[diagnosis, 'CDR=%s' % value] = len(diagnosis_df[diagnosis_df.cdr_global == float(value)])\n",
    "\n",
    "    return population_df\n",
    "\n",
    "population_df = characteristics_table(OASIS_df, OASIS_df)\n",
    "print(population_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Preprocessing\n",
    "#\n",
    "# Theoretically, the main advantage of deep learning methods is to be able to\n",
    "# work without extensive data preprocessing. However, as we have only a few\n",
    "# images to train the network in this lab session, the preprocessing here is\n",
    "# very extensive. More specifically, the images encountered:\n",
    "#\n",
    "# 1. Non-linear registration\n",
    "# 2. Segmentation of grey matter\n",
    "# 3. Conversion to tensor format (.pt)\n",
    "#\n",
    "# The preprocessed images all have the same size (121x145x121). You will find\n",
    "# below a Dataset that allow to browse easily the database.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from os import path\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, data_df, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.data_df = data_df\n",
    "        self.label_code = {\"AD\":1, \"CN\":0}\n",
    "\n",
    "        self.size = self[0]['image'].shape\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        diagnosis = self.data_df.loc[idx, 'diagnosis']\n",
    "        label = self.label_code[diagnosis]\n",
    "        \n",
    "        participant_id = self.data_df.loc[idx, 'participant_id']\n",
    "        session_id = self.data_df.loc[idx, 'session_id']\n",
    "        filename = participant_id + '_' + session_id + \\\n",
    "          '_T1w_segm-graymatter_space-Ixi549Space_modulated-off_probability.pt'\n",
    "\n",
    "        image = torch.load(path.join(self.img_dir, filename))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'label': label,\n",
    "                  'participant_id': participant_id,\n",
    "                  'session_id': session_id}\n",
    "        return sample\n",
    "\n",
    "    def train(self):\n",
    "        self.transform.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.transform.eval()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# To facilitate the training and avoid overfitting due to the limited amount of\n",
    "# data, the network won't use the full image but only a part of the image (size\n",
    "# 30x40x30) centered on a specific neuroanatomical region: the hippocampus\n",
    "# (HC).\n",
    "# This structure is known to be linked to memory, and is atrophied in the\n",
    "# majority of cases of Alzheimer's disease patients.\n",
    "#\n",
    "# To improve the training and reduce overfitting, a random shift was added to\n",
    "# the cropping function. This means that the bounding box around the\n",
    "# hippocampus may be shifted by a limited amount of voxels in each of the three\n",
    "# directions.\n",
    "\n",
    "class CropLeftHC(object):\n",
    "    \"\"\"Crops the left hippocampus of a MRI non-linearly registered to MNI\"\"\"\n",
    "    def __init__(self, random_shift=0):\n",
    "        self.random_shift = random_shift\n",
    "        self.train_mode = True\n",
    "    def __call__(self, img):\n",
    "        if self.train_mode:\n",
    "            x = random.randint(-self.random_shift, self.random_shift)\n",
    "            y = random.randint(-self.random_shift, self.random_shift)\n",
    "            z = random.randint(-self.random_shift, self.random_shift)\n",
    "        else:\n",
    "            x, y, z = 0, 0, 0\n",
    "        return img[:, 25 + x:55 + x,\n",
    "                   50 + y:90 + y,\n",
    "                   27 + z:57 + z].clone()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n",
    "class CropRightHC(object):\n",
    "    \"\"\"Crops the right hippocampus of a MRI non-linearly registered to MNI\"\"\"\n",
    "    def __init__(self, random_shift=0):\n",
    "        self.random_shift = random_shift\n",
    "        self.train_mode = True\n",
    "    def __call__(self, img):\n",
    "        if self.train_mode:\n",
    "            x = random.randint(-self.random_shift, self.random_shift)\n",
    "            y = random.randint(-self.random_shift, self.random_shift)\n",
    "            z = random.randint(-self.random_shift, self.random_shift)\n",
    "        else:\n",
    "            x, y, z = 0, 0, 0\n",
    "        return img[:, 65 + x:95 + x,\n",
    "                   50 + y:90 + y,\n",
    "                   27 + z:57 + z].clone()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Visualization\n",
    "#\n",
    "# Here we visualize the raw, preprocessed and cropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "eFpB23ybfOWg",
    "outputId": "e25fce32-1179-45ae-86a2-d79fe6a37beb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "subject = 'sub-OASIS10003'\n",
    "preprocessed_pt = torch.load('OASIS-1_dataset/preprocessed/%s_ses-M00_' % subject +\n",
    "                    'T1w_segm-graymatter_space-Ixi549Space_modulated-off_' +\n",
    "                    'probability.pt')\n",
    "raw_nii = nib.load('OASIS-1_dataset/raw/%s_ses-M00_T1w.nii.gz' % subject)\n",
    "raw_np = raw_nii.get_data()\n",
    "\n",
    "def show_slices(slices):\n",
    "    \"\"\" Function to display a row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "slice_0 = raw_np[80, :, :]\n",
    "slice_1 = raw_np[:, 130, :]\n",
    "slice_2 = raw_np[:, :, 60]\n",
    "show_slices([rotate(slice_0, 90), rotate(slice_1, 90), slice_2])\n",
    "plt.suptitle('Slice of raw image of subject %s' % subject)\n",
    "plt.show()\n",
    "\n",
    "slice_0 = preprocessed_pt[0, 60, :, :]\n",
    "slice_1 = preprocessed_pt[0, :, 72, :]\n",
    "slice_2 = preprocessed_pt[0, :, :, 60]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle('Center slices of preprocessed image of subject %s' % subject)\n",
    "plt.show()\n",
    "\n",
    "leftHC_pt = CropLeftHC()(preprocessed_pt)\n",
    "slice_0 = leftHC_pt[0, 15, :, :]\n",
    "slice_1 = leftHC_pt[0, :, 20, :]\n",
    "slice_2 = leftHC_pt[0, :, :, 15]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle('Center slices of left HC of subject %s' % subject)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Cross-validation\n",
    "#\n",
    "# In order to choose hyperparameters the set of images is divided into a\n",
    "# training set (80%) and a validation set (20%). The data split was performed\n",
    "# in order to ensure a similar distribution of diagnosis, age and sex between\n",
    "# the subjects of the training set and the subjects of the validation set.\n",
    "# Moreover the MMS distribution of each class is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "2v9DDGoQcFy0",
    "outputId": "b5b69fac-fb03-45dc-8ce3-2434b03b91ba"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('OASIS-1_dataset/tsv_files/lab_1/train.tsv', sep='\\t')\n",
    "valid_df = pd.read_csv('OASIS-1_dataset/tsv_files/lab_1/validation.tsv', sep='\\t')\n",
    "\n",
    "train_population_df = characteristics_table(train_df, OASIS_df)\n",
    "valid_population_df = characteristics_table(valid_df, OASIS_df)\n",
    "print(\"Train\")\n",
    "print(train_population_df)\n",
    "print()\n",
    "print(\"Validation\")\n",
    "print(valid_population_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model\n",
    "# We propose here to design a convolutional neural network that takes for input\n",
    "# a patch centered on the left hippocampus of size 30x40x30. The architecture\n",
    "# of the network was found using a Random Search on architecture + optimization\n",
    "# hyperparameters.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "484-30ZSd8CT"
   },
   "source": [
    "## Reminder on CNN layers\n",
    "\n",
    "In a CNN everything is called a layer though the operations layers perform\n",
    "are very different. You will find below a summary of the different operations\n",
    "that may be performed in a CNN.\n",
    "\n",
    "\n",
    "%% [markdown]\n",
    "### Feature maps\n",
    "\n",
    "The outputs of the layers in a convolutional network are called feature maps.\n",
    "Their size is written with the format: \n",
    "\n",
    "> `n_channels @ dim1 x dim2 x dim3`\n",
    "\n",
    "For a 3D CNN the dimension of the feature maps is actually 5D as the first\n",
    "dimension is the batch size. This dimension is added by the `DataLoader` of\n",
    "pytorch which stacks the 4D tensors computed by a `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "x1VVZU27iXCR",
    "outputId": "42b88c8e-a286-4501-c1ed-bb71788ab862"
   },
   "outputs": [],
   "source": [
    "img_dir = path.join('OASIS-1_dataset', 'preprocessed')\n",
    "batch_size=4\n",
    "\n",
    "example_dataset = MRIDataset(img_dir, OASIS_df, transform=CropLeftHC())\n",
    "example_dataloader = DataLoader(example_dataset, batch_size=batch_size, drop_last=True)\n",
    "for data in example_dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Shape of Dataset output\\n\", example_dataset[0]['image'].shape)\n",
    "print()\n",
    "print(\"Shape of DataLoader output\\n\", data['image'].shape)\n",
    "\n",
    "# %% [markdown]\n",
    "#\n",
    "# ### Convolutions (`nn.Conv3d`)\n",
    "#\n",
    "# The main arguments of this layer are the input channels, the output channels\n",
    "# (number of filters trained) and the size of the filter (or kernel). If an\n",
    "# integer `k` is given the kernel will be a cube of size `k`. It is possible to\n",
    "# construct rectangular kernels by entering a tuple (but this is very rare).\n",
    "#\n",
    "# You will find below an illustration of how a single filter produces its\n",
    "# output feature map by parsing the one feature map. The size of the output\n",
    "# feature map produced depends of the convolution parameters and can be\n",
    "# computed with the following formula:\n",
    "#\n",
    "# > $O_i = \\frac{I_i-k+2P}{S} + 1$\n",
    "#\n",
    "# *   $O_i$ the size of the output along the ith dimension\n",
    "# *   $I_i$ the size of the input along the ith dimension\n",
    "# *   $k$ the size of the kernel\n",
    "# *   $P$ the padding value\n",
    "# *   $S$ the stride value\n",
    "#\n",
    "# In the following example $\\frac{5-3+2*0}{1}+1 = 3$\n",
    "#\n",
    "# ![2D convolutional layer gif](https://drive.google.com/uc?id=166EuqiwIZkKPMOlVzA-v5WemJE2tDCES) \n",
    "#\n",
    "# To be able to parse all the feature maps of the input, one filter is actually\n",
    "# a 4D tensor of size `(input_channels, k, k, k)`. The ensemble of all the\n",
    "# filters included in one convolutional layer is then a 5D tensor stacking all\n",
    "# the filters of size `(output_channels, input_channels, k, k, k)`.\n",
    "#\n",
    "# Each filter is also associated to one bias value that is a scalar added to\n",
    "# all the feature maps it produces. Then the bias is a 1D vector of size\n",
    "# `output_channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "ETjl7kp17-IM",
    "outputId": "6b2987dd-850c-4598-fe3d-b8cb20a81e4b"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "conv_layer = nn.Conv3d(8, 16, 3)\n",
    "print('Weights shape\\n', conv_layer.weight.shape)\n",
    "print()\n",
    "print('Bias shape\\n', conv_layer.bias.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Batch Normalization (`nn.BatchNorm3d`)\n",
    "#\n",
    "# Learns to normalize feature maps according to [(Ioffe & Szegedy,\n",
    "# 2015)](https://arxiv.org/abs/1502.03167). The following formula is applied on\n",
    "# each feature map  $FM_i$:\n",
    "#\n",
    "# > $FM^{normalized}_i = \\frac{FM_i - mean(FM_i)}{\\sqrt{var(FM_i) + \\epsilon}} * \\gamma_i + \\beta_i$\n",
    "#\n",
    "# *   $\\epsilon$ is a hyperparameter of the layer (default=1e-05)\n",
    "# *   $\\gamma_i$ is the value of the scale for the ith channel (learnable parameter)\n",
    "# *   $\\beta_i$ is the value of the shift for the ith channel (learnable parameter)\n",
    "#\n",
    "# This layer does not have the same behaviour during training and evaluation,\n",
    "# this is why it is needed to put the model in evaluation mode in the test\n",
    "# function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "8QPASZehDHjc",
    "outputId": "6b9218de-4d3f-48fc-fb3f-63343f47f843"
   },
   "outputs": [],
   "source": [
    "batch_layer = nn.BatchNorm3d(16)\n",
    "print('Gamma value\\n', batch_layer.state_dict()['weight'].shape)\n",
    "print()\n",
    "print('Beta value\\n', batch_layer.state_dict()['bias'].shape)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Activation function (`nn.LeakyReLU`)\n",
    "#\n",
    "# In order to introduce non-linearity in the model, an activation function is\n",
    "# introduced after the convolutions. It is applied on all intensities\n",
    "# independently.\n",
    "#\n",
    "# The graph of the Leaky ReLU is displayed below, $\\alpha$ being a\n",
    "# hyperparameter of the layer (default=0.01):\n",
    "#\n",
    "# ![Leaky ReLU graph](https://sefiks.com/wp-content/uploads/2018/02/prelu.jpg?w=600)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Pooling function (`PadMaxPool3d`)\n",
    "#\n",
    "# The structure of the pooling layer is very similar to the convolutional\n",
    "# layer: a kernel is passing through the input with a defined size and stride.\n",
    "# However there is no learnable parameters in this layer, the kernel outputing\n",
    "# the maximum value of the part of the feature map it covers.\n",
    "#\n",
    "# Here is an example in 2D of the standard layer of pytorch `nn.MaxPool2d`:\n",
    "#\n",
    "# ![nn.MaxPool2d behaviour](https://drive.google.com/uc?id=1qh9M9r9mfpZeSD1VjOGQAl8zWqBLmcKz)\n",
    "#\n",
    "# We can observe that the last column may not be used depending on the size of\n",
    "# the kernel/input and stride value.\n",
    "#\n",
    "# This is why the custom module `PadMaxPool` was defined to pad the input in\n",
    "# order to exploit information from the whole feature map.\n",
    "#\n",
    "class PadMaxPool3d(nn.Module):\n",
    "    \"\"\"A MaxPooling module which deals with odd sizes with padding\"\"\"\n",
    "    def __init__(self, kernel_size, stride, return_indices=False, return_pad=False):\n",
    "        super(PadMaxPool3d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pool = nn.MaxPool3d(kernel_size, stride, return_indices=return_indices)\n",
    "        self.pad = nn.ConstantPad3d(padding=0, value=0)\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "\n",
    "    def set_new_return(self, return_indices=True, return_pad=True):\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "        self.pool.return_indices = return_indices\n",
    "\n",
    "    def forward(self, f_maps):\n",
    "        coords = [self.stride - f_maps.size(i + 2) % self.stride for i in range(3)]\n",
    "        for i, coord in enumerate(coords):\n",
    "            if coord == self.stride:\n",
    "                coords[i] = 0\n",
    "\n",
    "        self.pad.padding = (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output, indices = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, indices, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output, indices\n",
    "\n",
    "        else:\n",
    "            output = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Here is an illustration of `PadMaxPool` behaviour, a column is added to avoid\n",
    "# losing data:\n",
    "#\n",
    "# ![PadMaxPool behaviour](https://drive.google.com/uc?id=14R_LCTiV0N6ZXm-3wQCj_Gtc1LsXdQq_)\n",
    "#\n",
    "# Similarly, the formula to find the size of the output feature map is:\n",
    "#\n",
    "# > $O_i = ceil(\\frac{I_i-k+2P}{S}) + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout (`nn.Dropout`)\n",
    "\n",
    "The aim of a dropout layer is to replace a fixed proportion of the input\n",
    "values by 0 during training only.\n",
    "\n",
    "This layer does not have the same behaviour during training and evaluation,\n",
    "this is why it is needed to put the model in evaluation mode in the test\n",
    "function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "-0O3cCR7HGge",
    "outputId": "5e09b0ea-b9ae-42c5-f390-b51f4a8b07e5"
   },
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "input_tensor = torch.rand(10)\n",
    "output_tensor = dropout(input_tensor)\n",
    "print(\"Input \\n\", input_tensor)\n",
    "print()\n",
    "print(\"Output \\n\", output_tensor)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Fully-Connected Layers (`nn.Linear`)\n",
    "#\n",
    "# The fully connected layers take as input 2D vectors of size `(batch_size,\n",
    "# N)`. They have two mandatory arguments, the number of values per batch of the\n",
    "# input and the number of values per batch of the output.\n",
    "#\n",
    "# Each output neuron in a FC layer is a linear combination of the inputs + a\n",
    "# bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "um2OupjmGy94",
    "outputId": "42ef6dec-e7d5-40a5-8547-5d682afa539e"
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(16, 2)\n",
    "print(\"Weights shape \\n\", fc.weight.shape)\n",
    "print()\n",
    "print(\"Bias shape \\n\", fc.bias.shape)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# The max pooling module of pytorch does not deal with odd sizes, indeed when\n",
    "# the size is not a multiple of the stride (here 2) the last column is thrown.\n",
    "# To avoid this we use a custom module `PadMaxPool3d` instead of the standard\n",
    "# module `nn.MaxPooling3d`.\n",
    "\n",
    "class PadMaxPool3d(nn.Module):\n",
    "    \"\"\"A MaxPooling module which deals with odd sizes with padding\"\"\"\n",
    "    def __init__(self, kernel_size, stride, return_indices=False, return_pad=False):\n",
    "        super(PadMaxPool3d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pool = nn.MaxPool3d(kernel_size, stride, return_indices=return_indices)\n",
    "        self.pad = nn.ConstantPad3d(padding=0, value=0)\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "\n",
    "    def set_new_return(self, return_indices=True, return_pad=True):\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "        self.pool.return_indices = return_indices\n",
    "\n",
    "    def forward(self, f_maps):\n",
    "        coords = [self.stride - f_maps.size(i + 2) % self.stride for i in range(3)]\n",
    "        for i, coord in enumerate(coords):\n",
    "            if coord == self.stride:\n",
    "                coords[i] = 0\n",
    "\n",
    "        self.pad.padding = (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output, indices = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, indices, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output, indices\n",
    "\n",
    "        else:\n",
    "            output = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## TODO Network design\n",
    "# Construct here the network corresponding to the scheme and the following\n",
    "# description:\n",
    "#\n",
    "# ![Scheme of the network](https://drive.google.com/uc?id=1Qi-ictqudBX4ToBXzqT5w57RHrkn3MPR)\n",
    "#\n",
    "# The network includes 3 convolutional blocks composed by a convolutional layer\n",
    "# (kernel size = 3, padding = 1, stride = 1), a batch normalization, a\n",
    "# LeakyReLU activation and a MaxPooling layer. The 3 successive layers include\n",
    "# respectively 8, 16 and 32 filters.\n",
    "#\n",
    "# Then, the feature maps array is flattened in a 1D array to enter a\n",
    "# fully-connected layer. Between the convolutional and the fully-connected\n",
    "# layers, a dropout layer with a dropout rate of 0.5 is inserted. \n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        # Create the layers composing the network\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compose the forward operation using the layers defined in __init__\n",
    "        pass\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train & Test\n",
    "#\n",
    "# Complete the `train` method in order to iteratively update the weights of the\n",
    "# network.\n",
    "#\n",
    "# Here the model leading to the lowest loss on the training set at the end of\n",
    "# an epoch is returned, however we could choose instead the model leading to\n",
    "# the highest balanced accuracy, or the one obtained at the last iteration.\n",
    "#\n",
    "# In many studies of deep learning the validation set is used during training\n",
    "# to choose when the training should stop (early stopping) but also to retrieve\n",
    "# the best model (model selection).\n",
    "#\n",
    "# As we don't have any test set to evaluate the final model selected in an\n",
    "# unbiased way, we chose not to use the validation set in training in order to\n",
    "# limit the bias of the validation set. However you can choose to implement an\n",
    "# early stopping and / or model selection based on the validation set, but\n",
    "# remember that even if your results on the validation set are better, that\n",
    "# doesn't mean that this would be the case on an independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "1yVfkO-QvzfM",
    "outputId": "ff6c9524-cdba-4894-938e-0a2d94609f12"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    Method used to train a CNN\n",
    "    \n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "        optimizer: (torch.optim) an optimization algorithm\n",
    "        n_epochs: (int) number of epochs performed during training\n",
    "\n",
    "    Returns:\n",
    "        best_model: (nn.Module) the trained neural network\n",
    "    \"\"\"\n",
    "    best_model = deepcopy(model)\n",
    "    train_best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loader.dataset.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Complete the train iteration\n",
    "\n",
    "        _, train_metrics = test(model, train_loader, criterion)\n",
    "\n",
    "        print('Epoch %i: loss = %f, balanced accuracy = %f' \n",
    "              % (epoch, train_metrics['mean_loss'],\n",
    "                 train_metrics['balanced_accuracy']))\n",
    "\n",
    "        if train_metrics['mean_loss'] < train_best_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            train_best_loss = train_metrics['mean_loss']\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def test(model, data_loader, criterion):\n",
    "    \"\"\"\n",
    "    Method used to test a CNN\n",
    "    \n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "    \n",
    "    Returns:\n",
    "        results_df: (DataFrame) the label predicted for every subject\n",
    "        results_metrics: (dict) a set of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader.dataset.eval()\n",
    "    columns = [\"participant_id\", \"proba0\", \"proba1\",\n",
    "               \"true_label\", \"predicted_label\"]\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            images, labels = data['image'].cuda(), data['label'].cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            probs = nn.Softmax(dim=1)(outputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            for idx, sub in enumerate(data['participant_id']):\n",
    "                row = [sub,\n",
    "                       probs[idx, 0].item(), probs[idx, 1].item(),\n",
    "                       labels[idx].item(), predicted[idx].item()]\n",
    "                row_df = pd.DataFrame([row], columns=columns)\n",
    "                results_df = pd.concat([results_df, row_df])\n",
    "\n",
    "    results_metrics = compute_metrics(results_df.true_label.values, results_df.predicted_label.values)\n",
    "    results_df.reset_index(inplace=True, drop=True)\n",
    "    results_metrics['mean_loss'] = total_loss / len(data_loader.dataset)\n",
    "    \n",
    "    return results_df, results_metrics\n",
    "\n",
    "\n",
    "def compute_metrics(ground_truth, prediction):\n",
    "    \"\"\"Computes the accuracy, sensitivity, specificity and balanced accuracy\"\"\"\n",
    "    tp = np.sum((prediction == 1) & (ground_truth == 1))\n",
    "    tn = np.sum((prediction == 0) & (ground_truth == 0))\n",
    "    fp = np.sum((prediction == 1) & (ground_truth == 0))\n",
    "    fn = np.sum((prediction == 0) & (ground_truth == 1))\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    metrics_dict['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Sensitivity\n",
    "    if tp + fn != 0:\n",
    "        metrics_dict['sensitivity'] = tp / (tp + fn)\n",
    "    else:\n",
    "        metrics_dict['sensitivity'] = 0.0\n",
    "        \n",
    "    # Specificity\n",
    "    if fp + tn != 0:\n",
    "        metrics_dict['specificity'] = tn / (fp + tn)\n",
    "    else:\n",
    "        metrics_dict['specificity'] = 0.0\n",
    "        \n",
    "    metrics_dict['balanced_accuracy'] = (metrics_dict['sensitivity'] + metrics_dict['specificity']) / 2\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Train Classification with Left HC\n",
    "#\n",
    "# Here we will train a first network that will learn to perform the binary\n",
    "# classification AD vs CN on a cropped image around the left hippocampus.\n",
    "#\n",
    "# All hyperparameters may have an influence, but one of the most influent is\n",
    "# the learning rate that can lead to a poor convergence if it is too high or\n",
    "# low. Try different learning rate between $10 ^{-5}$ and $10 ^{-3}$ and\n",
    "# observe the differences of loss variations during training.\n",
    "#\n",
    "# To increase the training speed you can also increase the batch size. But be\n",
    "# careful, if the batch size becomes a non-negligible amount of the training\n",
    "# set it may have a negative impact on loss convergence [(Keskar et al,\n",
    "# 2016)](https://arxiv.org/abs/1609.04836). \n",
    "#\n",
    "# Construction of dataset objects\n",
    "\n",
    "img_dir = path.join('OASIS-1_dataset', 'preprocessed')\n",
    "transform = CropLeftHC(2)\n",
    "\n",
    "train_datasetLeftHC = MRIDataset(img_dir, train_df, transform=transform)\n",
    "valid_datasetLeftHC = MRIDataset(img_dir, valid_df, transform=transform)\n",
    "\n",
    "# Try different learning rates\n",
    "learning_rate = ...\n",
    "n_epochs = 30\n",
    "batch_size = 4\n",
    "\n",
    "# Put the network on GPU\n",
    "modelLeftHC = CustomNetwork().cuda()\n",
    "train_loaderLeftHC = DataLoader(train_datasetLeftHC, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "# A high batch size improves test speed\n",
    "valid_loaderLeftHC = DataLoader(valid_datasetLeftHC, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(modelLeftHC.parameters(), learning_rate)\n",
    "\n",
    "best_modelLeftHC = train(modelLeftHC, train_loaderLeftHC, criterion, optimizer, n_epochs)\n",
    "\n",
    "valid_resultsLeftHC_df, valid_metricsLeftHC = test(best_modelLeftHC, valid_loaderLeftHC, criterion)\n",
    "train_resultsLeftHC_df, train_metricsLeftHC = test(best_modelLeftHC, train_loaderLeftHC, criterion)\n",
    "print(valid_metricsLeftHC)\n",
    "print(train_metricsLeftHC)\n",
    "\n",
    "# %% [markdown]\n",
    "# If you obtained about 0.85 or more of balanced accuracy, there may be\n",
    "# something wrong... Are you absolutely sure that your dataset is unbiased?\n",
    "#\n",
    "# If you didn't remove the youngest subjects of OASIS, your dataset is biased\n",
    "# as the AD and CN participants do not have the same age distribution.\n",
    "# In practice people who come to the hospital for a diagnosis of Alzheimer's\n",
    "# disease all have about the same age (50 - 90). No one has Alzheimer's disease\n",
    "# at 20 ! Then you should check that the performance of the network is still\n",
    "# good for the old population only.\n",
    "\n",
    "# Check accuracy of old participants (age > 62 to match the minimum of AD age\n",
    "# distribution)\n",
    "valid_resultsLeftHC_df = valid_resultsLeftHC_df.merge(OASIS_df, how='left', on='participant_id', sort=False)\n",
    "valid_resultsLeftHC_old_df = valid_resultsLeftHC_df[(valid_resultsLeftHC_df.age_bl_x >= 62)]\n",
    "compute_metrics(valid_resultsLeftHC_old_df.true_label, valid_resultsLeftHC_old_df.predicted_label)\n",
    "\n",
    "# %% [markdown]\n",
    "# If this is not the case, you have to think again about your framework and\n",
    "# eventually retrain your network...\n",
    "\n",
    "# %%[markdown]\n",
    "# ## Train Classification with Right HC\n",
    "#\n",
    "# Another network can be trained on a cropped image around the right HC\n",
    "# network. The same hyperparameters as before may be reused.\n",
    "#\n",
    "# Construction of dataset objects\n",
    "\n",
    "transform = CropRightHC(2)\n",
    "\n",
    "train_datasetRightHC = MRIDataset(img_dir, train_df, transform=transform)\n",
    "valid_datasetRightHC = MRIDataset(img_dir, valid_df, transform=transform)\n",
    "\n",
    "learning_rate = ...\n",
    "n_epochs = 30\n",
    "batch_size = 4\n",
    "\n",
    "# Put the network on GPU\n",
    "modelRightHC = CustomNetwork().cuda()\n",
    "train_loaderRightHC = DataLoader(train_datasetRightHC, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loaderRightHC = DataLoader(valid_datasetRightHC, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(modelRightHC.parameters(), learning_rate)\n",
    "\n",
    "best_modelRightHC = train(modelRightHC, train_loaderRightHC, criterion, optimizer, n_epochs)\n",
    "\n",
    "valid_resultsRightHC_df, valid_metricsRightHC = test(best_modelRightHC, valid_loaderRightHC, criterion)\n",
    "train_resultsRightHC_df, train_metricsRightHC = test(best_modelRightHC, train_loaderRightHC, criterion)\n",
    "print(valid_metricsRightHC)\n",
    "print(train_metricsRightHC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DrQZBkufKM-"
   },
   "source": [
    "## Soft voting\n",
    "To increase the accuracy of our system the results of the two networks can be\n",
    "combined. Here we can give both hippocampi the same weight.\n",
    "\n",
    "def softvoting(df1, df2):\n",
    "    df1 = df1.set_index('participant_id', drop=True)\n",
    "    df2 = df2.set_index('participant_id', drop=True)\n",
    "    results_df = pd.DataFrame(index=df1.index.values,\n",
    "                              columns=['true_label', 'predicted_label', \n",
    "                                       'proba0', 'proba1'])\n",
    "    results_df.true_label = df1.true_label\n",
    "    # Compute predicted label and probabilities\n",
    "    results_df.proba1 = 0.5 * df1.proba1 + 0.5 * df2.proba1\n",
    "    results_df.proba0 = 0.5 * df1.proba0 + 0.5 * df2.proba0\n",
    "    results_df.predicted_label = (0.5 * df1.proba1 + 0.5 * df2.proba1 > 0.5).astype(int)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "valid_results = softvoting(valid_resultsLeftHC_df, valid_resultsRightHC_df)\n",
    "valid_metrics = compute_metrics(valid_results.true_label, valid_results.predicted_label)\n",
    "print(valid_metrics)\n",
    "\n",
    "\n",
    "%% [markdown]\n",
    "Keep in mind that the validation set was used to set the hyperparameters\n",
    "(learning rate, architecture), then validation metrics are biased. To have\n",
    "unbiased results the entire framework should be evaluated when all the\n",
    "hyperparameters are set on an independent set (test set).\n",
    "\n",
    "%% [markdown]\n",
    "# Clustering on AD & CN populations\n",
    "\n",
    "The classification results above were obtained in a supervised way:\n",
    "neurologists examine the participants of OASIS and gave a diagnosis depending\n",
    "on their clinical symptoms.\n",
    "\n",
    "However, this label is often inaccurate (Beach et al, 2012).\n",
    "Then an unsupervised framework can be interesting to check what can be found\n",
    "in data without being biased by a noisy label.\n",
    "\n",
    "%% [markdown]\n",
    "## Model\n",
    "\n",
    "A convenient architecture to extract features from an image with deep\n",
    "learning is the autoencoder (AE). This architecture is made of two parts:\n",
    "- the **encoder** which learns to compress the image in a smaller vector, the\n",
    "**code**. *It is composed of the same kind of operations than the\n",
    "convolutional part of the CNN seen before.*\n",
    "- the **decoder** which learns to reconstruct the original image from the\n",
    "code learnt by the encoder. *It is composed of the transposed version of the\n",
    "operations used in the encoder.*\n",
    "\n",
    "%% [markdown]\n",
    "You will find below `CropMaxUnpool3d` the transposed version of\n",
    "`PadMaxPool3d`.\n",
    "\n",
    "class CropMaxUnpool3d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(CropMaxUnpool3d, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool3d(kernel_size, stride)\n",
    "\n",
    "    def forward(self, f_maps, indices, padding=None):\n",
    "        output = self.unpool(f_maps, indices)\n",
    "        if padding is not None:\n",
    "            x1 = padding[4]\n",
    "            y1 = padding[2]\n",
    "            z1 = padding[0]\n",
    "            output = output[:, :, x1::, y1::, z1::]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "%% [markdown]\n",
    "To facilitate the reconstruction process, the pooling layers in the encoder\n",
    "return the position of the values that were the maximum. Hence the unpooling\n",
    "layer can replace the maximum values at the right place in the 2x2x2 sub-cube\n",
    "of the feature map. They also indicate if some zero padding was applied to\n",
    "the feature map so that the unpooling layer can correctly crop their output\n",
    "feature map.  \n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Initial size (30, 40, 30)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, 3, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (15, 20, 15)\n",
    "            \n",
    "            nn.Conv3d(8, 16, 3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (8, 10, 8)\n",
    "            \n",
    "            nn.Conv3d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (4, 5, 4)\n",
    "\n",
    "            nn.Conv3d(32, 1, 1),\n",
    "            # Size (4, 5, 4)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(1, 32, 1),\n",
    "            # Size (4, 5, 4)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(32, 16, 3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            # Size (8, 10, 8)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(16, 8, 3, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            # Size (15, 20, 15)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(8, 1, 3, padding=1),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid()\n",
    "            # Size (30, 40, 30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        indices_list = []\n",
    "        pad_list = []\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, PadMaxPool3d):\n",
    "                x, indices, pad = layer(x)\n",
    "                indices_list.append(indices)\n",
    "                pad_list.append(pad)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        code = x.view(x.size(0), -1)\n",
    "        for layer in self.decoder:\n",
    "            if isinstance(layer, CropMaxUnpool3d):\n",
    "                x = layer(x, indices_list.pop(), pad_list.pop())\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return code, x\n",
    "\n",
    "\n",
    "%% [markdown]\n",
    "## Train Autoencoder\n",
    "\n",
    "The training function of the autoencoder is very similar to the training\n",
    "function of the CNN. The main difference is that the loss is not computed by\n",
    "comparing the output with the diagnosis values using the cross-entropy loss,\n",
    "but with the original image using for example the Mean Squared Error (MSE)\n",
    "loss.\n",
    "\n",
    "def trainAE(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    Method used to train an AutoEncoder\n",
    "    \n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "        optimizer: (torch.optim) an optimization algorithm\n",
    "        n_epochs: (int) number of epochs performed during training\n",
    "\n",
    "    Returns:\n",
    "        best_model: (nn.Module) the trained neural network.\n",
    "    \"\"\"\n",
    "    best_model = deepcopy(model)\n",
    "    train_best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loader.dataset.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            @TODO\n",
    "            # Complete the training function in a similar way\n",
    "            # than for the CNN classification training.\n",
    "\n",
    "        mean_loss = testAE(model, train_loader, criterion)\n",
    "\n",
    "        print('Epoch %i: loss = %f' % (epoch, mean_loss))\n",
    "\n",
    "        if mean_loss < train_best_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            train_best_loss = mean_loss\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def testAE(model, data_loader, criterion):\n",
    "    \"\"\"\n",
    "    Method used to test an AutoEncoder\n",
    "    \n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "    \n",
    "    Returns:\n",
    "        results_df: (DataFrame) the label predicted for every subject\n",
    "        results_metrics: (dict) a set of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader.dataset.eval()\n",
    "    columns = [\"participant_id\", \"proba0\", \"proba1\",\n",
    "               \"true_label\", \"predicted_label\"]\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            images, labels = data['image'].cuda(), data['label'].cuda()\n",
    "            _, outputs = model((images))\n",
    "            loss = criterion(outputs, images)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader.dataset) / np.product(data_loader.dataset.size)\n",
    "\n",
    "\n",
    "learning_rate = 10**-2\n",
    "n_epochs = 30\n",
    "batch_size = 4\n",
    "\n",
    "AELeftHC = AutoEncoder().cuda()\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(AELeftHC.parameters(), learning_rate)\n",
    "\n",
    "best_AELeftHC = trainAE(AELeftHC, train_loaderLeftHC, criterion, optimizer, n_epochs)\n",
    "\n",
    "%% [markdown]\n",
    "## Visualization\n",
    "\n",
    "The simplest way to check if the AE training went well is to visualize the\n",
    "output and compare it to the original image seen by the autoencoder.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "subject = 'sub-OASIS10003'\n",
    "preprocessed_pt = torch.load('OASIS-1_dataset/preprocessed/%s_ses-M00_' % subject +\n",
    "                    'T1w_segm-graymatter_space-Ixi549Space_modulated-off_' +\n",
    "                    'probability.pt')\n",
    "input_pt = CropLeftHC()(preprocessed_pt).unsqueeze(0).cuda()\n",
    "_, output_pt = best_AELeftHC(input_pt)\n",
    "\n",
    "\n",
    "slice_0 = input_pt[0, 0, 15, :, :].cpu()\n",
    "slice_1 = input_pt[0, 0, :, 20, :].cpu()\n",
    "slice_2 = input_pt[0, 0, :, :, 15].cpu()\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle('Center slices of the input image of subject %s' % subject)\n",
    "plt.show()\n",
    "\n",
    "slice_0 = output_pt[0, 0, 15, :, :].cpu().detach()\n",
    "slice_1 = output_pt[0, 0, :, 20, :].cpu().detach()\n",
    "slice_2 = output_pt[0, 0, :, :, 15].cpu().detach()\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle('Center slices of the output image of subject %s' % subject)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "%% [markdown]\n",
    "## Clustering\n",
    "\n",
    "Now that the AE extracted the most salient parts of the image in a smaller\n",
    "vector, the features obtained can be used for clustering.\n",
    "\n",
    "Here we give an example with the Gaussian Mixture Model (GMM) of\n",
    "scikit-learn. To use it we first need to concat the features and the labels\n",
    "of all the subjects in two matrices *X* and *Y*. This is what is done in\n",
    "`compute_dataset_features` method.\n",
    "\n",
    "def compute_dataset_features(data_loader, model):\n",
    "\n",
    "    concat_codes = torch.Tensor().cuda()\n",
    "    concat_labels = torch.LongTensor()\n",
    "    concat_names = []\n",
    "\n",
    "    for data in data_loader:\n",
    "      image = data['image'].cuda()\n",
    "      labels = data['label']\n",
    "      names = data['participant_id']\n",
    "\n",
    "      code, _ = model(image)\n",
    "      concat_codes = torch.cat([concat_codes, code.squeeze(1)], 0)\n",
    "      concat_labels = torch.cat([concat_labels, labels])\n",
    "      concat_names = concat_names + names\n",
    "\n",
    "    concat_codes_np = concat_codes.cpu().detach().numpy()\n",
    "    concat_labels_np = concat_labels.numpy()\n",
    "    concat_names = np.array(concat_names)[:, np.newaxis]\n",
    "\n",
    "    return concat_codes_np, concat_labels_np, concat_names\n",
    "\n",
    "\n",
    "train_codes, train_labels, names = compute_dataset_features(train_loaderBothHC, best_AEBothHC)\n",
    "\n",
    "%% [markdown]\n",
    "Then the model will fit the training codes and build two clusters. The labels\n",
    "found in this unsupervised way can be compared to the true labels.\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "components = 2\n",
    "model = mixture.GaussianMixture(components)\n",
    "model.fit(X)\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "metrics = compute_metrics(Y, Y_pred)\n",
    "print(metrics)\n",
    "\n",
    "%% [markdown]\n",
    "The accuracy may not be very good, this could mean that the framework\n",
    "classified another characteristic that the one you tried to target.\n",
    "\n",
    "What is actually expected is that the clustering differenciation is made on\n",
    "the level of atrophy, which is mostly correlated to the age but also to the\n",
    "disease stage (we can model it with the MMS score).\n",
    "\n",
    "data_np = np.concatenate([names, train_codes,\n",
    "                          train_labels[:, np.newaxis],\n",
    "                          train_predict[:, np.newaxis]], axis=1)\n",
    "columns = ['feature %i' % i for i in range(train_codes.shape[1])]\n",
    "columns = ['participant_id'] + columns + ['true_label', 'predicted_label']\n",
    "data_df = pd.DataFrame(data_np, columns=columns).set_index('participant_id')\n",
    "\n",
    "merged_df = data_df.merge(OASIS_df.set_index('participant_id'), how='inner', on='participant_id')\n",
    "\n",
    "plt.title('Clustering values according to age and MMS score')\n",
    "for component in range(n_components):\n",
    "    predict_df = merged_df[merged_df.predicted_label == str(component)]\n",
    "    plt.plot(predict_df['age_bl'], predict_df['MMS'], 'o', label=\"cluster %i\" % component)\n",
    "plt.legend()\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('MMS')\n",
    "plt.show()\n",
    "\n",
    "%% [markdown] id=\"u6zujwJw1cyd\"\n",
    "You can try to improve this clustering by adding the codes obtained on the\n",
    "right hippocampus, perform further dimension reduction or remove age effect\n",
    "like in [(Moradi et al,\n",
    "2015)](https://www.researchgate.net/publication/266374876_Machine_learning_framework_for_early_MRI-based_Alzheimer%27s_conversion_prediction_in_MCI_subjects)..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
